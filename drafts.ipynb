{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:50:40.800459Z",
     "start_time": "2024-05-06T19:50:36.301028Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:50:40.805668Z",
     "start_time": "2024-05-06T19:50:40.801340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Autoencoder for numerical data\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))"
   ],
   "id": "8c24b8dd5293c100",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:50:51.550613Z",
     "start_time": "2024-05-06T19:50:40.806360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the results list\n",
    "results = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Create a dictionary for each dataset\n",
    "    result = {'dataset_name': file}\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "    result['data'] = dataset\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=2)\n",
    "    result['pca_embeddings'] = pca.fit_transform(dataset)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=2)\n",
    "    result['t-SNE_embeddings'] = tsne.fit_transform(dataset)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=2)\n",
    "    result['umap_embeddings'] = umap.fit_transform(dataset)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 32\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        result['autoencoder_embeddings'] = autoencoder.encoder(dataset_torch).numpy()\n",
    "\n",
    "\n",
    "    # Clusterize the embeddings\n",
    "    kmeans = KMeans(n_clusters=10)\n",
    "    result['pca_cluster'] = kmeans.fit_predict(result['pca_embeddings'])\n",
    "    result['t-SNE_cluster'] = kmeans.fit_predict(result['t-SNE_embeddings'])\n",
    "    result['umap_cluster'] = kmeans.fit_predict(result['umap_embeddings'])\n",
    "    result['autoencoder_cluster'] = kmeans.fit_predict(result['autoencoder_embeddings'])\n",
    "\n",
    "    # Append the result to the results list\n",
    "    results.append(result)"
   ],
   "id": "402353fe16abc79f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m\n\u001B[1;32m     35\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mBCELoss()\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Convert the dataset to PyTorch tensors\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m dataset_torch \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Normalize the data to be between 0 and 1\u001B[39;00m\n\u001B[1;32m     41\u001B[0m dataset_torch \u001B[38;5;241m=\u001B[39m (dataset_torch \u001B[38;5;241m-\u001B[39m dataset_torch\u001B[38;5;241m.\u001B[39mmin()) \u001B[38;5;241m/\u001B[39m (dataset_torch\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m-\u001B[39m dataset_torch\u001B[38;5;241m.\u001B[39mmin())\n",
      "\u001B[0;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result",
   "id": "58f4610e093fe434",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
