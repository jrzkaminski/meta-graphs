{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:03:43.989897Z",
     "start_time": "2024-05-20T17:03:38.563844Z"
    },
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24b8dd5293c100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:03:43.994956Z",
     "start_time": "2024-05-20T17:03:43.990952Z"
    }
   },
   "source": [
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402353fe16abc79f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:09:23.265081Z",
     "start_time": "2024-05-20T17:03:43.995645Z"
    }
   },
   "source": [
    "# Initialize the results list\n",
    "results = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Create a dictionary for each dataset\n",
    "    result = {'dataset_name': file}\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "    result['data'] = dataset\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # Handle missing values by filling them with the mean of each column\n",
    "    dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert boolean columns to integers\n",
    "    for col in dataset.select_dtypes(include='bool').columns:\n",
    "        dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "    # Fill any remaining NaN values that could result from the conversion\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=3)\n",
    "    result['pca_embeddings'] = pca.fit_transform(dataset)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=3)\n",
    "    result['t-SNE_embeddings'] = tsne.fit_transform(dataset)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=3)\n",
    "    result['umap_embeddings'] = umap.fit_transform(dataset)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 32\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        result['autoencoder_embeddings'] = autoencoder.encoder(dataset_torch).numpy()\n",
    "\n",
    "    # Clusterize the embeddings\n",
    "    kmeans = KMeans(n_clusters=10)\n",
    "    result['pca_cluster'] = kmeans.fit_predict(result['pca_embeddings'])\n",
    "    result['t-SNE_cluster'] = kmeans.fit_predict(result['t-SNE_embeddings'])\n",
    "    result['umap_cluster'] = kmeans.fit_predict(result['umap_embeddings'])\n",
    "    result['autoencoder_cluster'] = kmeans.fit_predict(result['autoencoder_embeddings'])\n",
    "\n",
    "    # Append the result to the results list\n",
    "    results.append(result)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9e4db82f6045e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:24:56.075597Z",
     "start_time": "2024-05-20T17:24:55.690785Z"
    }
   },
   "source": [
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb048d7be56f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:09:23.268240Z",
     "start_time": "2024-05-20T17:09:23.266279Z"
    }
   },
   "source": [
    "# # Load the DAGs from txt files\n",
    "# dag_files = []\n",
    "# for root, dirs, files in os.walk('dags'):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.txt'):\n",
    "#             dag_files.append(os.path.join(root, file))\n",
    "# \n",
    "# # Initialize the graph embeddings dictionary\n",
    "# graph_embeddings = {}\n",
    "# \n",
    "# for file in dag_files:\n",
    "#     with open(file, 'r') as f:\n",
    "#         edges = [tuple(line.strip().split()) for line in f]\n",
    "# \n",
    "#     # Create a directed graph\n",
    "#     G = nx.DiGraph()\n",
    "#     G.add_edges_from(edges)\n",
    "# \n",
    "#     # Convert node labels to integers\n",
    "#     mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "#     G = nx.relabel_nodes(G, mapping)\n",
    "# \n",
    "#     # Generate embeddings using Node2Vec\n",
    "#     node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "#     model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "# \n",
    "#     # Save the embeddings for each node\n",
    "#     embeddings = {node: model.wv[str(node)] for node in G.nodes()}\n",
    "#     graph_embeddings[file] = embeddings\n",
    "# \n",
    "# # Now `graph_embeddings` contains the node embeddings for each DAG"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e86f27e51c19b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:12:41.951150Z",
     "start_time": "2024-05-20T17:12:41.910694Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Function to load graphs from txt files\n",
    "def load_graphs_from_txt(directory):\n",
    "    graph_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                graph_files.append(os.path.join(root, file))\n",
    "    return graph_files\n",
    "\n",
    "# Load graphs from the directory\n",
    "graph_files = load_graphs_from_txt('graphs')\n",
    "\n",
    "# Initialize the graph embeddings dictionary\n",
    "graph_embeddings = []\n",
    "\n",
    "for file in graph_files:\n",
    "    with open(file, 'r') as f:\n",
    "        edges = [tuple(line.strip().split()) for line in f]\n",
    "    \n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Create a mapping from node labels to numeric indices\n",
    "    mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    \n",
    "    # Add dummy node features\n",
    "    for i in G.nodes:\n",
    "        G.nodes[i]['feature'] = [1.0] * 10\n",
    "    \n",
    "    data = from_networkx(G)\n",
    "    \n",
    "    # Convert node features to tensor\n",
    "    data.x = torch.tensor([G.nodes[i]['feature'] for i in G.nodes], dtype=torch.float)\n",
    "    \n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = GCN(num_node_features=10, hidden_dim=16, num_classes=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, torch.tensor([0 for _ in G.nodes], dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Get GCN embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.conv1(data.x, data.edge_index).numpy()\n",
    "    \n",
    "    # Append embeddings to the list\n",
    "    graph_embeddings.append((file, embeddings))\n",
    "\n",
    "# Flatten the embeddings for clustering and plotting\n",
    "all_embeddings = np.vstack([emb for _, emb in graph_embeddings])\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(all_embeddings)\n",
    "\n",
    "# Create a list of labels for the embeddings\n",
    "embedding_labels = []\n",
    "for i, (file, embeddings) in enumerate(graph_embeddings):\n",
    "    embedding_labels.extend([f\"{file}-{j}\" for j in range(embeddings.shape[0])])\n",
    "\n",
    "# Plot embeddings with Plotly\n",
    "def plot_3d_scatter(embeddings, clusters, labels, title):\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings[:, 0], \n",
    "        y=embeddings[:, 1], \n",
    "        z=embeddings[:, 2], \n",
    "        color=clusters, \n",
    "        title=title,\n",
    "        text=labels\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Plot GCN embeddings\n",
    "plot_3d_scatter(all_embeddings, clusters, embedding_labels, \"GCN Embeddings for Graphs\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db346a50914eb8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:31:00.397092Z",
     "start_time": "2024-05-20T17:28:53.271613Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Function to load graphs from txt files\n",
    "def load_graphs_from_txt(directory):\n",
    "    graph_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                graph_files.append(os.path.join(root, file))\n",
    "    return graph_files\n",
    "\n",
    "# Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data/bnlearn_data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize the lists\n",
    "data_embeddings = []\n",
    "graph_embeddings = []\n",
    "file_names = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # Handle missing values by filling them with the mean of each column\n",
    "    dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert boolean columns to integers\n",
    "    for col in dataset.select_dtypes(include='bool').columns:\n",
    "        dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "    # Fill any remaining NaN values that could result from the conversion\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embeddings = pca.fit_transform(dataset)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=3)\n",
    "    tsne_embeddings = tsne.fit_transform(dataset)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=3)\n",
    "    umap_embeddings = umap.fit_transform(dataset)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 3\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        autoencoder_embeddings = autoencoder.encoder(dataset_torch).numpy()\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    data_embeddings.append((file, pca_embeddings, tsne_embeddings, umap_embeddings, autoencoder_embeddings))\n",
    "\n",
    "    # Load the corresponding graph\n",
    "    graph_file = file.replace('.csv', '.txt')\n",
    "    with open(graph_file, 'r') as f:\n",
    "        edges = [tuple(line.strip().split()) for line in f]\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Create a mapping from node labels to numeric indices\n",
    "    mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    # Add dummy node features\n",
    "    for i in G.nodes:\n",
    "        G.nodes[i]['feature'] = [1.0] * 10\n",
    "\n",
    "    data = from_networkx(G)\n",
    "\n",
    "    # Convert node features to tensor\n",
    "    data.x = torch.tensor([G.nodes[i]['feature'] for i in G.nodes], dtype=torch.float)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = GCN(num_node_features=10, hidden_dim=16, num_classes=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, torch.tensor([0 for _ in G.nodes], dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get GCN embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gcn_embeddings = model.conv1(data.x, data.edge_index).numpy()\n",
    "\n",
    "    # Append embeddings to the graph embeddings list\n",
    "    graph_embeddings.append(gcn_embeddings)\n",
    "    file_names.append(file)\n",
    "\n",
    "# Flatten the graph embeddings for clustering\n",
    "all_graph_embeddings = np.vstack(graph_embeddings)\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "graph_clusters = kmeans.fit_predict(all_graph_embeddings)\n",
    "\n",
    "# Create a mapping from file name to cluster\n",
    "file_to_cluster = {file_names[i]: graph_clusters[i] for i in range(len(file_names))}\n",
    "\n",
    "# Plot embeddings with Plotly\n",
    "def plot_3d_scatter(embeddings, file, title):\n",
    "    cluster = file_to_cluster[file]\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings[:, 0], \n",
    "        y=embeddings[:, 1], \n",
    "        z=embeddings[:, 2], \n",
    "        color=[cluster] * len(embeddings), \n",
    "        title=title\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Plot PCA, t-SNE, UMAP, and Autoencoder embeddings\n",
    "for file, pca_emb, tsne_emb, umap_emb, autoencoder_emb in data_embeddings:\n",
    "    plot_3d_scatter(pca_emb, file, f\"PCA Embeddings: {file}\")\n",
    "    plot_3d_scatter(tsne_emb, file, f\"t-SNE Embeddings: {file}\")\n",
    "    plot_3d_scatter(umap_emb, file, f\"UMAP Embeddings: {file}\")\n",
    "    plot_3d_scatter(autoencoder_emb, file, f\"Autoencoder Embeddings: {file}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "736d109283f5207d",
   "metadata": {
    "metadata": {},
    "ExecuteTime": {
     "end_time": "2024-05-20T17:48:05.369729Z",
     "start_time": "2024-05-20T17:46:05.191084Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Function to load graphs from txt files\n",
    "def load_graphs_from_txt(directory):\n",
    "    graph_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                graph_files.append(os.path.join(root, file))\n",
    "    return graph_files\n",
    "\n",
    "# Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data/bnlearn_data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize the lists\n",
    "data_embeddings = []\n",
    "graph_embeddings = []\n",
    "file_names = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # Handle missing values by filling them with the mean of each column\n",
    "    dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert boolean columns to integers\n",
    "    for col in dataset.select_dtypes(include='bool').columns:\n",
    "        dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "    # Fill any remaining NaN values that could result from the conversion\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embeddings = pca.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=3)\n",
    "    tsne_embeddings = tsne.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=3)\n",
    "    umap_embeddings = umap.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 3\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        autoencoder_embeddings = autoencoder.encoder(dataset_torch).mean(axis=0).numpy()\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    data_embeddings.append((file, pca_embeddings, tsne_embeddings, umap_embeddings, autoencoder_embeddings))\n",
    "\n",
    "    # Load the corresponding graph\n",
    "    graph_file = file.replace('.csv', '.txt')\n",
    "    with open(graph_file, 'r') as f:\n",
    "        edges = [tuple(line.strip().split()) for line in f]\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Create a mapping from node labels to numeric indices\n",
    "    mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    # Add dummy node features\n",
    "    for i in G.nodes:\n",
    "        G.nodes[i]['feature'] = [1.0] * 10\n",
    "\n",
    "    data = from_networkx(G)\n",
    "\n",
    "    # Convert node features to tensor\n",
    "    data.x = torch.tensor([G.nodes[i]['feature'] for i in G.nodes], dtype=torch.float)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = GCN(num_node_features=10, hidden_dim=16, num_classes=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, torch.tensor([0 for _ in G.nodes], dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get GCN embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gcn_embeddings = model.conv1(data.x, data.edge_index).mean(axis=0).numpy()\n",
    "\n",
    "    # Append embeddings to the graph embeddings list\n",
    "    graph_embeddings.append(gcn_embeddings)\n",
    "    file_names.append(file)\n",
    "\n",
    "# Convert to numpy arrays for clustering\n",
    "data_embeddings_np = np.array([emb[1:] for emb in data_embeddings])  # Skip the file name\n",
    "graph_embeddings_np = np.array(graph_embeddings)\n",
    "\n",
    "# Flatten the graph embeddings for clustering\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "graph_clusters = kmeans.fit_predict(graph_embeddings_np)\n",
    "\n",
    "# Create a mapping from file name to cluster\n",
    "file_to_cluster = {file_names[i]: graph_clusters[i] for i in range(len(file_names))}\n",
    "\n",
    "# Extract data embeddings and assign clusters\n",
    "pca_embs = np.array([emb[1] for emb in data_embeddings])\n",
    "tsne_embs = np.array([emb[2] for emb in data_embeddings])\n",
    "umap_embs = np.array([emb[3] for emb in data_embeddings])\n",
    "autoencoder_embs = np.array([emb[4] for emb in data_embeddings])\n",
    "\n",
    "# Plot embeddings with Plotly and save them as HTML files\n",
    "def save_3d_scatter(embeddings, clusters, title, filename):\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings[:, 0], \n",
    "        y=embeddings[:, 1], \n",
    "        z=embeddings[:, 2], \n",
    "        color=clusters, \n",
    "        title=title\n",
    "    )\n",
    "    fig.write_html(filename)\n",
    "\n",
    "# Save PCA, t-SNE, UMAP, and Autoencoder embeddings plots\n",
    "save_3d_scatter(pca_embs, graph_clusters, \"PCA Embeddings\", \"result_plots/pca_embeddings.html\")\n",
    "save_3d_scatter(tsne_embs, graph_clusters, \"t-SNE Embeddings\", \"result_plots/tsne_embeddings.html\")\n",
    "save_3d_scatter(umap_embs, graph_clusters, \"UMAP Embeddings\", \"result_plots/umap_embeddings.html\")\n",
    "save_3d_scatter(autoencoder_embs, graph_clusters, \"Autoencoder Embeddings\", \"result_plots/autoencoder_embeddings.html\")\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T18:08:26.628346Z",
     "start_time": "2024-05-20T18:06:33.859025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Function to load graphs from txt files\n",
    "def load_graphs_from_txt(directory):\n",
    "    graph_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                graph_files.append(os.path.join(root, file))\n",
    "    return graph_files\n",
    "\n",
    "# Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data/bnlearn_data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize the lists\n",
    "data_embeddings = []\n",
    "graph_embeddings = []\n",
    "file_names = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # Handle missing values by filling them with the mean of each column\n",
    "    dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert boolean columns to integers\n",
    "    for col in dataset.select_dtypes(include='bool').columns:\n",
    "        dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "    # Fill any remaining NaN values that could result from the conversion\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embeddings = pca.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=3)\n",
    "    tsne_embeddings = tsne.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=3)\n",
    "    umap_embeddings = umap.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 3\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        autoencoder_embeddings = autoencoder.encoder(dataset_torch).mean(axis=0).numpy()\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    data_embeddings.append((file, pca_embeddings, tsne_embeddings, umap_embeddings, autoencoder_embeddings))\n",
    "\n",
    "    # Load the corresponding graph\n",
    "    graph_file = file.replace('.csv', '.txt')\n",
    "    with open(graph_file, 'r') as f:\n",
    "        edges = [tuple(line.strip().split()) for line in f]\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Create a mapping from node labels to numeric indices\n",
    "    mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    # Add dummy node features\n",
    "    for i in G.nodes:\n",
    "        G.nodes[i]['feature'] = [1.0] * 10\n",
    "\n",
    "    data = from_networkx(G)\n",
    "\n",
    "    # Convert node features to tensor\n",
    "    data.x = torch.tensor([G.nodes[i]['feature'] for i in G.nodes], dtype=torch.float)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = GCN(num_node_features=10, hidden_dim=16, num_classes=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, torch.tensor([0 for _ in G.nodes], dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get GCN embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gcn_embeddings = model.conv1(data.x, data.edge_index).mean(axis=0).numpy()\n",
    "\n",
    "    # Append embeddings to the graph embeddings list\n",
    "    graph_embeddings.append(gcn_embeddings)\n",
    "    file_names.append(file)\n",
    "\n",
    "# Convert to numpy arrays for clustering\n",
    "data_embeddings_np = np.array([emb[1:] for emb in data_embeddings])  # Skip the file name\n",
    "graph_embeddings_np = np.array(graph_embeddings)\n",
    "\n",
    "# Flatten the graph embeddings for clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "graph_clusters = kmeans.fit_predict(graph_embeddings_np)\n",
    "\n",
    "# Create a mapping from file name to cluster\n",
    "file_to_cluster = {file_names[i]: graph_clusters[i] for i in range(len(file_names))}\n",
    "\n",
    "# Extract data embeddings and assign clusters\n",
    "pca_embs = np.array([emb[1] for emb in data_embeddings])\n",
    "tsne_embs = np.array([emb[2] for emb in data_embeddings])\n",
    "umap_embs = np.array([emb[3] for emb in data_embeddings])\n",
    "autoencoder_embs = np.array([emb[4] for emb in data_embeddings])\n",
    "dataset_names = np.array([emb[0] for emb in data_embeddings])\n",
    "\n",
    "# Plot embeddings with Plotly and save them as HTML files\n",
    "def save_3d_scatter(embeddings, clusters, dataset_names, title, filename):\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings[:, 0], \n",
    "        y=embeddings[:, 1], \n",
    "        z=embeddings[:, 2], \n",
    "        color=clusters, \n",
    "        text=dataset_names, \n",
    "        title=title\n",
    "    )\n",
    "    fig.write_html(filename)\n",
    "\n",
    "# Save PCA, t-SNE, UMAP, and Autoencoder embeddings plots\n",
    "save_3d_scatter(pca_embs, graph_clusters, dataset_names, \"PCA Embeddings\", \"result_plots/pca_embeddings.html\")\n",
    "save_3d_scatter(tsne_embs, graph_clusters, dataset_names, \"t-SNE Embeddings\", \"result_plots/tsne_embeddings.html\")\n",
    "save_3d_scatter(umap_embs, graph_clusters, dataset_names, \"UMAP Embeddings\", \"result_plots/umap_embeddings.html\")\n",
    "save_3d_scatter(autoencoder_embs, graph_clusters, dataset_names, \"Autoencoder Embeddings\",\n",
    "                \"result_plots/autoencoder_embeddings.html\")\n"
   ],
   "id": "bce41ecd53116810",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T18:17:21.965617Z",
     "start_time": "2024-05-20T18:15:02.930455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Function to load graphs from txt files\n",
    "def load_graphs_from_txt(directory):\n",
    "    graph_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                graph_files.append(os.path.join(root, file))\n",
    "    return graph_files\n",
    "\n",
    "# Load the datasets\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk('data/bnlearn_data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize the lists\n",
    "data_embeddings = []\n",
    "graph_embeddings = []\n",
    "file_names = []\n",
    "\n",
    "for file in data_files:\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file)\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    # Handle missing values by filling them with the mean of each column\n",
    "    dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert boolean columns to integers\n",
    "    for col in dataset.select_dtypes(include='bool').columns:\n",
    "        dataset[col] = dataset[col].astype(int)\n",
    "\n",
    "    # Fill any remaining NaN values that could result from the conversion\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    # PCA for numerical data\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embeddings = pca.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # t-SNE for numerical data\n",
    "    tsne = TSNE(n_components=3)\n",
    "    tsne_embeddings = tsne.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # UMAP for numerical data\n",
    "    umap = UMAP(n_components=3)\n",
    "    umap_embeddings = umap.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # ICA for numerical data\n",
    "    ica = FastICA(n_components=3)\n",
    "    ica_embeddings = ica.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # Factor Analysis for numerical data\n",
    "    fa = FactorAnalysis(n_components=3)\n",
    "    fa_embeddings = fa.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # Isomap for numerical data\n",
    "    isomap = Isomap(n_components=3)\n",
    "    isomap_embeddings = isomap.fit_transform(dataset).mean(axis=0)\n",
    "\n",
    "    # Define the size of the encoded representations\n",
    "    encoding_dim = 3\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Autoencoder(dataset.shape[1], encoding_dim)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = Adam(autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    dataset_torch = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    dataset_torch = (dataset_torch - dataset_torch.min()) / (dataset_torch.max() - dataset_torch.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(50):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(dataset_torch)\n",
    "        loss = criterion(outputs, dataset_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Generate the embeddings\n",
    "    with torch.no_grad():\n",
    "        autoencoder_embeddings = autoencoder.encoder(dataset_torch).mean(axis=0).numpy()\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    data_embeddings.append((file, pca_embeddings, tsne_embeddings, umap_embeddings, ica_embeddings, fa_embeddings, isomap_embeddings, autoencoder_embeddings))\n",
    "\n",
    "    # Load the corresponding graph\n",
    "    graph_file = file.replace('.csv', '.txt')\n",
    "    with open(graph_file, 'r') as f:\n",
    "        edges = [tuple(line.strip().split()) for line in f]\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Create a mapping from node labels to numeric indices\n",
    "    mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    # Add dummy node features\n",
    "    for i in G.nodes:\n",
    "        G.nodes[i]['feature'] = [1.0] * 10\n",
    "\n",
    "    data = from_networkx(G)\n",
    "\n",
    "    # Convert node features to tensor\n",
    "    data.x = torch.tensor([G.nodes[i]['feature'] for i in G.nodes], dtype=torch.float)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = GCN(num_node_features=10, hidden_dim=16, num_classes=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, torch.tensor([0 for _ in G.nodes], dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Get GCN embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gcn_embeddings = model.conv1(data.x, data.edge_index).mean(axis=0).numpy()\n",
    "\n",
    "    # Append embeddings to the graph embeddings list\n",
    "    graph_embeddings.append(gcn_embeddings)\n",
    "    file_names.append(file)\n",
    "\n",
    "# Convert to numpy arrays for clustering\n",
    "data_embeddings_np = np.array([emb[1:] for emb in data_embeddings])  # Skip the file name\n",
    "graph_embeddings_np = np.array(graph_embeddings)\n",
    "\n",
    "# Flatten the graph embeddings for clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "graph_clusters = kmeans.fit_predict(graph_embeddings_np)\n",
    "\n",
    "# Create a mapping from file name to cluster\n",
    "file_to_cluster = {file_names[i]: graph_clusters[i] for i in range(len(file_names))}\n",
    "\n",
    "# Extract data embeddings and assign clusters\n",
    "pca_embs = np.array([emb[1] for emb in data_embeddings])\n",
    "tsne_embs = np.array([emb[2] for emb in data_embeddings])\n",
    "umap_embs = np.array([emb[3] for emb in data_embeddings])\n",
    "ica_embs = np.array([emb[4] for emb in data_embeddings])\n",
    "fa_embs = np.array([emb[5] for emb in data_embeddings])\n",
    "isomap_embs = np.array([emb[6] for emb in data_embeddings])\n",
    "autoencoder_embs = np.array([emb[7] for emb in data_embeddings])\n",
    "dataset_names = np.array([emb[0] for emb in data_embeddings])\n",
    "\n",
    "# Plot embeddings with Plotly and save them as HTML files\n",
    "def save_3d_scatter(embeddings, clusters, dataset_names, title, filename):\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings[:, 0], \n",
    "        y=embeddings[:, 1], \n",
    "        z=embeddings[:, 2], \n",
    "        color=clusters, \n",
    "        text=dataset_names, \n",
    "        title=title\n",
    "    )\n",
    "    fig.write_html(filename)\n",
    "\n",
    "# Save PCA, t-SNE, UMAP, ICA, Factor Analysis, Isomap, and Autoencoder embeddings plots\n",
    "save_3d_scatter(pca_embs, graph_clusters, dataset_names, \"PCA Embeddings\", \"result_plots/pca_embeddings.html\")\n",
    "save_3d_scatter(tsne_embs, graph_clusters, dataset_names, \"t-SNE Embeddings\", \"result_plots/tsne_embeddings.html\")\n",
    "save_3d_scatter(umap_embs, graph_clusters, dataset_names, \"UMAP Embeddings\", \"result_plots/umap_embeddings.html\")\n",
    "save_3d_scatter(ica_embs, graph_clusters, dataset_names, \"ICA Embeddings\", \"result_plots/ica_embeddings.html\")\n",
    "save_3d_scatter(fa_embs, graph_clusters, dataset_names, \"Factor Analysis Embeddings\", \"result_plots/fa_embeddings.html\")\n",
    "save_3d_scatter(isomap_embs, graph_clusters, dataset_names, \"Isomap Embeddings\", \"result_plots/isomap_embeddings.html\")\n",
    "save_3d_scatter(autoencoder_embs, graph_clusters, dataset_names, \"Autoencoder Embeddings\",\n",
    "                \"result_plots/autoencoder_embeddings.html\")\n"
   ],
   "id": "d5d55f030b237b86",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "4ef89da7f66c911c",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
